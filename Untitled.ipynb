{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87196a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "#from lie_utils import SE3, SE3Exp\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "import numpy as np\n",
    "\n",
    "from nerfstudio.cameras.rays import Frustums, RayBundle, RaySamples\n",
    "from nerfstudio.field_components.field_heads import FieldHeadNames\n",
    "from nerfstudio.pipelines.base_pipeline import VanillaPipeline\n",
    "from nerfstudio.utils.eval_utils import eval_setup\n",
    "from nerfstudio.cameras.cameras import Cameras\n",
    "\n",
    "import tinycudann as tcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf027c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config_and_model(base_path, dataset):\n",
    "    dataset_path = os.path.join(base_path, dataset, \"nerfacto\")\n",
    "    # get all directories in the dataset path\n",
    "    dirs = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "    # make sure there is only one directory\n",
    "    assert len(dirs) == 1\n",
    "    # get the directory name\n",
    "    model_dir = dirs[0]\n",
    "    # load the config file\n",
    "    config_path = Path(os.path.join(dataset_path, model_dir, \"config.yml\"))\n",
    "    # load the model and return the trainer_config, pipeline, and ckpt_path\n",
    "    return eval_setup(config_path)\n",
    "\n",
    "def load_model(config_path):\n",
    "    if(isinstance(config_path, Path)):\n",
    "        return eval_setup(config_path)\n",
    "    else:\n",
    "        return load_model(Path(config_path))\n",
    "    \n",
    "def load_training_poses(dataset_dir, transforms_json=\"transforms.json\"):\n",
    "    transforms_json_path = os.path.join(dataset_dir, transforms_json)\n",
    "    with open(transforms_json_path, \"r\") as f:\n",
    "        transforms_json = json.load(f)\n",
    "    # get the camera parameters\n",
    "    cx = torch.Tensor([transforms_json[\"cx\"]])\n",
    "    cy = torch.Tensor([transforms_json[\"cy\"]])\n",
    "    fx = torch.Tensor([transforms_json[\"fl_x\"]])\n",
    "    fy = torch.Tensor([transforms_json[\"fl_y\"]])\n",
    "    width = transforms_json[\"w\"]\n",
    "    height = transforms_json[\"h\"]\n",
    "\n",
    "    # get the camera poses\n",
    "    frames = transforms_json[\"frames\"]\n",
    "    poses = []\n",
    "    times = []\n",
    "    meta_names = []\n",
    "    for frame in frames:\n",
    "        tf_matrix = frame[\"transform_matrix\"]\n",
    "        pose = torch.tensor(tf_matrix).reshape(4, 4)\n",
    "        poses.append(pose)\n",
    "        meta_names.append(frame[\"file_path\"].replace(\"images/\",\"\"))\n",
    "        times.append(frame[\"time\"])\n",
    "    poses = torch.stack(poses, dim=0)\n",
    "\n",
    "    return (cx, cy, fx, fy, width, height), poses, meta_names, times\n",
    "\n",
    "def apply_dataparser_transforms(poses, dataparser_tf_json):\n",
    "    with(open(dataparser_tf_json, \"r\") as f):\n",
    "        dp_tf = json.load(f)\n",
    "    dataparser_tf_matrix = torch.eye(4)\n",
    "    dataparser_tf_matrix[0:3, :] = torch.tensor(dp_tf[\"transform\"])\n",
    "    dataparser_tf_scale = torch.tensor(dp_tf[\"scale\"])\n",
    "    # apply the dataparser transform to the poses\n",
    "    poses = torch.matmul(dataparser_tf_matrix, poses)\n",
    "    poses[..., :3, 3] *= dataparser_tf_scale\n",
    "    return poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2017dafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[22:28:08] </span>Auto image downscale factor of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>                                                 <a href=\"file:///home/stanlew/src/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/stanlew/src/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#349\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">349</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[22:28:08]\u001b[0m\u001b[2;36m \u001b[0mAuto image downscale factor of \u001b[1;36m1\u001b[0m                                                 \u001b]8;id=520343;file:///home/stanlew/src/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=651149;file:///home/stanlew/src/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#349\u001b\\\u001b[2m349\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Skipping <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> files in dataset split train.                                         <a href=\"file:///home/stanlew/src/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/stanlew/src/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#170\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split train.                                         \u001b]8;id=731107;file:///home/stanlew/src/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=637272;file:///home/stanlew/src/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#170\u001b\\\u001b[2m170\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Skipping <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span> files in dataset split test.                                          <a href=\"file:///home/stanlew/src/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">nerfstudio_dataparser.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/stanlew/src/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#170\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mSkipping \u001b[1;36m0\u001b[0m files in dataset split test.                                          \u001b]8;id=850468;file:///home/stanlew/src/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py\u001b\\\u001b[2mnerfstudio_dataparser.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=148060;file:///home/stanlew/src/nerfstudio/nerfstudio/data/dataparsers/nerfstudio_dataparser.py#170\u001b\\\u001b[2m170\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up training dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up training dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">900</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m900\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Warning: If you run out of memory, try reducing the number of images to sample from.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33mWarning: If you run out of memory, try reducing the number of images to sample from.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Setting up evaluation dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Setting up evaluation dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Caching all <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span> images.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Caching all \u001b[1;36m100\u001b[0m images.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading latest checkpoint from load_dir\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Loading latest checkpoint from load_dir\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Done loading checkpoint from outputs/v2_multifacto/multifacto/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">07</span>-15_145950/nerfstudio_models/step-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">000029999.</span>ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Done loading checkpoint from outputs/v2_multifacto/multifacto/\u001b[1;36m2023\u001b[0m-\u001b[1;36m07\u001b[0m-15_145950/nerfstudio_models/step-\u001b[1;36m000029999.\u001b[0mckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_config, pipeline, ckpt_path = load_model(\"/home/stanlew/src/nerfstudio/outputs/v2_multifacto/multifacto/2023-07-15_145950/config.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f390c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_params, poses, fnames, times = load_training_poses(\"/media/stanlew/Data/franka_arm_data/v2_multifacto\")\n",
    "poses = apply_dataparser_transforms(poses, \"/home/stanlew/src/nerfstudio/outputs/v2_multifacto/multifacto/2023-07-15_145950/dataparser_transforms.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "153fafd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([640.]),\n",
       " tensor([480.]),\n",
       " tensor([831.3844]),\n",
       " tensor([831.3844]),\n",
       " 1280,\n",
       " 960)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cam_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22a00b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pose_0_6.png'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e9171c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cx, cy, fx, fy, width, height = cam_params\n",
    "cams_0 = Cameras(camera_to_worlds=poses[6, :3, :].expand(4,-1,-1).to(pipeline.device),fx=fx,fy=fy,cx=cx,cy=cy,width=width,height=height)\n",
    "cams_0.times = torch.Tensor([[0.0],[1.0],[2.0],[3.0]]).to(cams_0.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bea4cffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4e62072a2d4081afce9e15063ada5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs = []\n",
    "for camera_idx in tqdm(range(cams_0.size)):\n",
    "    ray_bundle = cams_0.generate_rays(camera_indices=camera_idx)\n",
    "    outputs = pipeline.model.get_outputs_for_camera_ray_bundle(ray_bundle)\n",
    "    img = outputs['rgb'].cpu().numpy()\n",
    "    imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27bdf016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show image 1 in the notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "for i in range(len(imgs)):\n",
    "    cv.imwrite(f\"{i}_arm.png\", (imgs[i]*255)[...,::-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2171b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
